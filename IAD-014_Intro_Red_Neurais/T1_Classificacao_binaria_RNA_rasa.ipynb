{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho #1 - Classificação binária de pontos no plano com RNA rasa\n",
    "\n",
    "Nese trabalho iremos construir uma RNA rasa com uma única camada intermediária para classificar pontos de duas classes diferentes no plano. O objetivo desse trabalho é entender como funciona uma RNA e o seu treinamento usando o método do Gradiente Descendente.\n",
    "\n",
    "**Nesse trabalho você irá apreender como:**\n",
    "\n",
    "- Implementar uma RNA rasa para realizar classificação binária\n",
    "- Calcular a função de custo logistica (entropia cruzada)\n",
    "- Implementar a propagação para frente e a retro-propagação usando uma codificação simples sem vetorização nos exemplos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coloque aqui o seu nome\n",
    "\n",
    "Aluno:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Pacotes ##\n",
    "\n",
    "Em primeiro lugar é necessário importar alguns pacotes do Python que serão usados nesse trabalho:\n",
    "- [numpy](www.numpy.org) pacote de cálculo científico com Python\n",
    "- [sklearn](http://scikit-learn.org/stable/) fornece ferramentes eficientes e simples para análise de dados, usada nesse trabalho para gerar os dados de treinamento\n",
    "- [matplotlib](http://matplotlib.org) biblioteca para gerar gráficos em Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importação dos pacotes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sklearn.datasets\n",
    "\n",
    "np.random.seed(1) # define uma semente para geração de números aleatórios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Conjunto de dados ##\n",
    "\n",
    "Execute a célula abaixo para carregar o conjunto de dados de pontos no plano com um formato de de duas luas novas entrelaçadas nas variáveis `X`e `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define número de exemplos e chama função para gerar os dados\n",
    "m = 600\n",
    "data = sklearn.datasets.make_moons(n_samples=m, noise=.2)\n",
    "\n",
    "# Recupera dados de entrada e de saída do conjunto de dados\n",
    "X, Y = data\n",
    "X, Y = X.T, Y.reshape(1, Y.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para fazer um gráfico dos dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualização dos dados\n",
    "plt.figure(figsize=(6, 4.5))\n",
    "plt.scatter(X[0, :], X[1, :], c=Y.ravel(), s=40, cmap=plt.cm.Spectral)\n",
    "ax = plt.axes()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #1:\n",
    "\n",
    "Os dados consistem em:\n",
    "    \n",
    "- um tensor Numpy (matriz) `X`  que contém as coordenadas dos pontos  (x1, x2) para todos os exemplos\n",
    "- um tensor Numpy (vetor) `Y` que contém as classes (vermelho:0, azul:1) para todos os exemplos\n",
    "\n",
    "Vamos primeiramente analisar os dados.\n",
    "\n",
    "Na célula abaixo crie um código que verifica quantos exemplos de treinamento existem e as dimensões (`shape`) das variavéis  `X` and `Y`. \n",
    "\n",
    "**Dica**: Como obter as dimensões de um tensor numpy? [(help)](https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.shape.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: Verificar dimensões do dados\n",
    "\n",
    "### COMEÇE AQUI ### (≈ 3 linhas)\n",
    "# \n",
    "### TERMINE AQUI ###\n",
    "\n",
    "print ('A dimensão de X é: ' + str(shape_X))\n",
    "print ('A dimensão de Y é: ' + str(shape_Y))\n",
    "print ('Existem m = %d exemplos de treinamento' % (m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "    A dimensão de X é: (2, 400)\n",
    "    A dimensão de Y é: (1, 400)\n",
    "    Existem m = 400 exemplos de treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Codificação da RNA\n",
    "\n",
    "### 3.1 - Definição da estrutura da RNA ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #2: \n",
    "\n",
    "Na célula abaixo modifique a função `layer_sizes` para definir três variáveis:\n",
    "\n",
    "- `n_x` = número de entradas de cada exemplo\n",
    "- `n_h` = número de neurônios da camada intermediária\n",
    "- `n_y` = número de saídas da RNA\n",
    "\n",
    "**Dica**: use as dimensões de `X` e `Y` para achar `n_x` e `n_y`. Além disso defina o número de neurônios da camada intermediária como sendo igual a 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: dimensões da RNA\n",
    "\n",
    "def layer_sizes(X, Y):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    X = conjunto de dados de entrada (dimensão: número de entradas, numero de exemplos))\n",
    "    Y = classes dos dados (dimensão: número de saídas, numero de exemplos)\n",
    "    \n",
    "    Retorna:\n",
    "    n_x = número de entradas\n",
    "    n_h = número de neurônios da camada escondida\n",
    "    n_y = número de saídas\n",
    "    \"\"\"\n",
    "    ### COMECE AQUI ### (≈ 3 linHAS)\n",
    "    #\n",
    "    ### TERMINE AQUI ###\n",
    "    \n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para testar a sua função `layer_sizes`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_x, n_h, n_y = layer_sizes(X, Y)\n",
    "print(\"Número de entradas: n_x = \", n_x)\n",
    "print(\"Número de neurônios da camada escondida: n_h = \", n_h)\n",
    "print(\"Número de saídas: n_y = \", n_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada:**\n",
    "\n",
    "    Número de entradas: n_x = 2\n",
    "    Número de neurônios da camada escondida: n_h = 4\n",
    "    Número de saídas: n_y = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - Initialização dos parâmetros ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #3: \n",
    "\n",
    "Implemente a função `inicializa_parametros()` na célula abaixo.\n",
    "\n",
    "**Instruções**:\n",
    "- Garanta que as dimensões dos seus parâmetros esteja correta. Veja as notas de aula.\n",
    "- Os pesos das ligações são inicializados com números aleatórios pequenos.\n",
    "- Multiplique os números aleatórios gerados por 0,01 para ter números pequenos.\n",
    "- Os vieses dos neurônios são inicilizados com zeros.\n",
    "- Use a função `np.random.random` para gerar números aleatórios com distribuição uniforme. Multiplique os números aleatórios por 0,01 para ter números pequenos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: inicialização dos parâmetros da RNA\n",
    "\n",
    "def inicializa_parametros(n_x, n_h, n_y):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    n_x = número de entradas\n",
    "    n_h = número de neurônios da camada escondida\n",
    "    n_y = número de saídas\n",
    "    \n",
    "    Retorna:\n",
    "    W1 = matriz de pesos de dimensão (n_h, n_x)\n",
    "    b1 = vetor de vieses de dimensão (n_h, 1)\n",
    "    W2 = matriz de pesos de dimensão (n_y, n_h)\n",
    "    b2 = vetor de vieses de dimensão (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(2) # define semente para geração de números aleatórios de forma a uniformizar os resultados.\n",
    "    \n",
    "    ### COMECE AQUI ### (≈ 4 linhas)\n",
    "    #\n",
    "    ### TERMINE AQUIE ###\n",
    "    \n",
    "    assert (W1.shape == (n_h, n_x))\n",
    "    assert (b1.shape == (n_h, 1))\n",
    "    assert (W2.shape == (n_y, n_h))\n",
    "    assert (b2.shape == (n_y, 1))\n",
    "    \n",
    "    return (W1, b1, W2, b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para testar a sua função "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = inicializa_parametros(n_x, n_h, n_y)\n",
    "print(\"W1 = \", W1)\n",
    "print(\"b1 = \", b1)\n",
    "print(\"W2 = \", W2)\n",
    "print(\"b2 = \", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "    W1 =  [[0.00435995 0.00025926]\n",
    "     [0.00549662 0.00435322]\n",
    "     [0.00420368 0.00330335]\n",
    "     [0.00204649 0.00619271]]\n",
    "    b1 =  [[0.]\n",
    "     [0.]\n",
    "     [0.]\n",
    "     [0.]]\n",
    "    W2 =  [[0.00299655 0.00266827 0.00621134 0.00529142]]\n",
    "    b2 =  [[0.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 - Propagação para frente ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #4: \n",
    "\n",
    "Implemente a função `sigmoide()` para usá-la como função de ativação da camada de saída da rede.\n",
    "\n",
    "Essa função tem que estar preparada para receber um tensor como entrade de retornar um tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: função sigmoide\n",
    "\n",
    "def sigmoide(x):\n",
    "    \"\"\"\n",
    "    Argumentos: x = tensor de entrada\n",
    "    Retorna: s = sigmoide(x)\n",
    "    \"\"\"\n",
    "    \n",
    "    ### COMECE AQUI ### (≈ 1 linha)\n",
    "    #\n",
    "    ### TERMINE AQUIE ###\n",
    "    \n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para testar a sua função `sigmoide()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = np.linspace(-1.0, 1.0, num=5)\n",
    "s = sigmoide(t)\n",
    "print('Vetor de entrada =', t)\n",
    "print('Sigmoide =', s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada:**\n",
    "\n",
    "    Vetor de entrada = [-1.  -0.5  0.   0.5  1. ]\n",
    "    Sigmoide = [0.26894142 0.37754067 0.5        0.62245933 0.73105858]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #5: \n",
    "\n",
    "Implemente a função `forward_propagation()` que processa um único exemplo de treinamento. \n",
    "\n",
    "**Instruções**:\n",
    "\n",
    "- Como função de ativação da camada de saída use a função `sigmoid()` que você criou no exercício #4. \n",
    "- Como função de ativação da camada intermediária use a função `np.tanh()`, que faz parte da bilioteca Numpy.\n",
    "- Codifique a propagação para frente, ou seja, calcule $z^{[1]}, a^{[1]}, z^{[2]}$ and $a^{[2]}$ para cada exemplo do conjunto de dados de treinamento.\n",
    "    \n",
    "Para auxiliar, as equações que implementam a propagação para frente vistas em aula estão repetidas abaixo. As equações não vetorizadas nos exemplos devem ser utilizadas no seu programa.\n",
    "\n",
    "$$\\boldsymbol{z}^{[1](i)} = \\boldsymbol{W}^{[1]}\\boldsymbol{x}^{(i)} + \\boldsymbol{b}^{[1]}$$\n",
    "\n",
    "$$\\boldsymbol{a}^{[1](i)} = g^{[1]}(\\boldsymbol{z}^{[1](i)})$$\n",
    "\n",
    "$$z^{[2](i)} = \\boldsymbol{W}^{[2]}\\boldsymbol{a}^{[1](i)} + b^{[2]}$$\n",
    "  \n",
    "$$a^{[2](i)} = g^{[2]}(z^{[2](i)})$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: propagação para frente para cada exemplo de treinamento\n",
    "\n",
    "def forward_propagation(x, W1, b1, W2, b2):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    x = dados de entrada de um exemplo com dimensão (n_x, 1)\n",
    "    W1 = matriz de pesos de dimensão (n_h, n_x)\n",
    "    b1 = vetor de vieses de dimensão (n_h, 1)\n",
    "    W2 = matriz de pesos de dimensão (n_y, n_h)\n",
    "    b2 = vetor de vieses de dimensão (n_y, 1)\n",
    "    \n",
    "    Retorna:\n",
    "    z1 = estados dos neurônios da camada intermediária de dimensão (n_h, 1)\n",
    "    a1 = ativações dos neurônios da camada intermediária de dimensão (n_h, 1)\n",
    "    z2 = estado do neurônio da camada de saída de dimensão (n_y ,1)\n",
    "    a2 = ativação do neurônio da camada de saída (saída da rede) de dimensão (n_y ,1)\n",
    "    \"\"\"\n",
    "    # Garante que dimensões dos dados de entrada são de fato um vetor de nx linhas e uma coluna\n",
    "    n_x = x.shape[0]\n",
    "    x = np.reshape(x, (n_x, 1))\n",
    "       \n",
    "    # Implemente a propagação para frente para calcula A2, cuidado com as dimensões nas multiplicações\n",
    "    ### COMECE AQUI ### (≈ 4 linhas)\n",
    "    #z1 \n",
    "    #a1 \n",
    "    #z2 \n",
    "    #a2 \n",
    "    ### TERMINE AQUI ###\n",
    "    \n",
    "    # Verifica dimensão de a2\n",
    "    assert(a2.shape == (1, x.shape[1]))\n",
    "    \n",
    "    return (z1, a1, z2, a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para testar a sua função `forward_propagation()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z1, a1, z2, a2 = forward_propagation(X[:,0], W1, b1, W2, b2)\n",
    "\n",
    "# Nota: usaremos a média somente para verificar os resultados.\n",
    "print('z1 =', z1)\n",
    "print('a1 =', a1)\n",
    "print('z2 =', z2)\n",
    "print('a2 =', a2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "    z1 = [[ 0.00396995]\n",
    "     [ 0.00239743]\n",
    "     [ 0.00185025]\n",
    "     [-0.00206823]]\n",
    "    a1 = [[ 0.00396993]\n",
    "     [ 0.00239742]\n",
    "     [ 0.00185025]\n",
    "     [-0.00206823]]\n",
    "    z2 = [[1.88417269e-05]]\n",
    "    a2 = [[0.50000471]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 - Função de erro ####\n",
    "\n",
    "Dado que a saída da RNA, $a^{[2]}$, já foi calculada e está na variável `a2`, que contém a saída $a^{[2](i)}$ de um exemplo de treinamento, a função de erro logística, conforme visto na aula, é calculada da seguinte forma:\n",
    "\n",
    "$$L = - \\large{(} \\small y^{(i)}\\log\\left(a^{[2] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[2] (i)}\\right) \\large{)} \\small$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #6: \n",
    "\n",
    "Implemente a função `logistica()` para calcular $L$. Use a função numpy `np.log` da biblioteca numpy para calcular o logarítmo neperiano de um número real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: cálculo da função de erro logística\n",
    "\n",
    "def logistica(a2, y):\n",
    "    \"\"\"\n",
    "    Calcula o custo entropia-cruzada\n",
    "    \n",
    "    Argumentos:\n",
    "    a2 = saída da RNA (escalar)\n",
    "    y = classe real do exemplo (escalar)\n",
    "    \n",
    "    Retorna:\n",
    "    erro = função logística\n",
    "    \"\"\"\n",
    "    \n",
    "    # Calcule o custo entropia-cruzada\n",
    "    ### COMECE AQUIE ### (≈ 1 linha)\n",
    "    #\n",
    "    ### TERMINE AQUI ###\n",
    "    \n",
    "    erro = np.squeeze(erro) # para ter certeza de que as dimensões estão corretas\n",
    "\n",
    "    return erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para testar a sua função `logistica()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Erro = \" + str(logistica(a2, Y[0][0])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "    Erro = 0.6931377597408849"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.5 - Retro-propagação ####\n",
    "\n",
    "Usando os resultados da propagação para frente para um exemplo de treinamento, pode-se implementar a retro propagação para esse exemplo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #7: \n",
    "\n",
    "Implemente a função `backward_propagation()`.\n",
    "\n",
    "**Instruções**:\n",
    "A retro propagação é a parte mais difícil de se calcular nas RNAs. Para auxiliar, as equações que implementam a retro-propagação vistas em aula estão repetidas abaixo. As equações não vetorizadas nos exemplos devem ser utilizadas no seu programa.\n",
    "\n",
    "$$dz^{[2](i)} = a^{[2](i)} - y^{(i)}$$\n",
    "  \n",
    "$$d\\boldsymbol{W}^{[2](i)} = dz^{[2](i)} \\boldsymbol{a}^{[1](i)T}$$\n",
    "\n",
    "$$db^{[2](i)} += dz^{[2](i)}$$\n",
    "\n",
    "$$d\\boldsymbol{z}^{[1](i)} = \\boldsymbol{W}^{[2]T}dz^{[2](i)}*\\frac{dg^{[1]}(\\boldsymbol{z}^{[1](i)})}{dz}$$\n",
    "\n",
    "$$d\\boldsymbol{W}^{[1](i)} = d\\boldsymbol{z}^{[1](i)} \\boldsymbol{x}^{(i)T}$$ \n",
    " \n",
    "$$d\\boldsymbol{b}^{[1](i)} = d\\boldsymbol{z}^{[1](i)}$$ \n",
    " \n",
    "\n",
    "- Note que o símbolo \"$*$\" denota multiplicação elemento por elemento.\n",
    "\n",
    "\n",
    "- Dicas: \n",
    "    - Para calcular $d\\boldsymbol{z}^{[1](i)}$ é necessário calcular $\\frac{dg^{[1]}(\\boldsymbol{z}^{[1](i)})}{dz}$.\n",
    "    - Como $g^{[1]}(.)$ é a função de ativação `tanh` e $a^{[1](i)} = g^{[1]}(z^{[1](i)})$, então \n",
    "    $\\frac{dg^{[1]}(z^{[1](i)})}{dz} = 1 - (a^{[1](i)})^2$. \n",
    "    - Portanto, pode-se calcular \n",
    "    $\\frac{dg^{[1]}(z^{[1](i)})}{dz}$ usando `(1 - np.power(a1, 2))`.\n",
    "    \n",
    "    \n",
    "- Note que no caso dessa função de retro-propagação você não precisa acumular as somas dos gradientes, pois esse cálculo é feito para um único exemplo de treinamento. A somatória é realizada posteriormente.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: retro-propagação\n",
    "\n",
    "def backward_propagation(x, y, z1, a1, z2, a2, W2):\n",
    "    \"\"\"\n",
    "    Implemente a retro-propagação usando as equações acima.\n",
    "    \n",
    "    Argumentos:\n",
    "    x = entrada de um exemplo com dimensão (2, 1)\n",
    "    y = saída da classe real de um exemplo (escalar)\n",
    "    z1 = estados dos neurônios da camada intermediária de dimensão (n_h, 1)\n",
    "    a1 = ativações dos neurônios da camada intermediária e dimensão (n_h, 1)\n",
    "    z2 = estado do neurônio da camada de saída de dimensão (n_y ,1)\n",
    "    a2 = ativação do neurônio da camada de saída de dimensão (n_y ,1)\n",
    "    W2 = matriz de pesos da camada de saída de dimensão (n_y, n_h)\n",
    "    \n",
    "    Retorna:\n",
    "    dW1 = matriz de gradientes dos pesos de dimensão para um exemplo de treinamento (n_h, n_x)\n",
    "    db1 = vetor de gradientes dos vieses de dimensão para um exemplo de treinamento (n_h, 1)\n",
    "    dW2 = matriz de gradientes dos pesos de dimensão para um exemplo de treinamento (n_y, n_h)\n",
    "    db2 = vetor de gradientes dos vieses de dimensão para um exemplo de treinamento (n_y, 1)\n",
    "    \"\"\"\n",
    "        \n",
    "    # Garante que dimensões de x estão corretas\n",
    "    n_x = x.shape[0]\n",
    "    x = np.reshape(x, (n_x, 1))\n",
    "    \n",
    "    \n",
    "    # Retro-propagação: calcule dW1, db1, dW2, db2. \n",
    "    ### COMECE AQUI ### (≈ 6 linhas correspondendo às 6 equações acima)\n",
    "    #dz2 \n",
    "    #dW2 \n",
    "    #db2 \n",
    "    #dz1 \n",
    "    #dW1\n",
    "    #db1 \n",
    "    ### TERMINE AQUI ###\n",
    "    \n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para testar a sua função `backward_propagation()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dW1, db1, dW2, db2 = backward_propagation(X[:,0], Y[0][0], z1, a1, z2, a2, W2)\n",
    "print (\"dW1 = \", dW1)\n",
    "print (\"db1 = \", db1)\n",
    "print (\"dW2 = \", dW2)\n",
    "print (\"db2 = \", db2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "    dW1 =  [[-0.00142191  0.00097027]\n",
    "     [-0.00126616  0.00086399]\n",
    "     [-0.00294743  0.00201124]\n",
    "     [-0.0025109   0.00171337]]\n",
    "    db1 =  [[-0.00149824]\n",
    "     [-0.00133412]\n",
    "     [-0.00310563]\n",
    "     [-0.00264567]]\n",
    "    dW2 =  [[-0.00198495 -0.0011987  -0.00092512  0.0010341 ]]\n",
    "    db2 =  [[-0.49999529]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.6 - Atualização dos parâmetros ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #8:\n",
    "\n",
    "Implemente a atualização dos parâmetros. Deve-se usar `dJdW1`, `dJdb1`, `dJdW2` e `dJdb2` para atualizar `W1`, `b1`, `W2` e `b2`.\n",
    "\n",
    "Equação geral do gradiente descendente: \n",
    "\n",
    "$$\\theta = \\theta - \\alpha \\frac{\\partial J }{ \\partial \\theta }$$\n",
    "\n",
    "onde $\\alpha$ é a taxa de aprendizagem e $\\theta$ representa um parâmetro genérico da rede."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: atualização dos parâmetros\n",
    "\n",
    "def update_parameters(W1, b1, W2, b2, dJdW1, dJdb1, dJdW2, dJdb2, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Atualização dos parâmetros usando a regra do gradiente descendente\n",
    "    \n",
    "    Argumentos:\n",
    "    W1 = matriz de pesos de dimensão (n_h, n_x)\n",
    "    b1 = vetor de vieses de dimensão (n_h, 1)\n",
    "    W2 = matriz de pesos de dimensão (n_y, n_h)\n",
    "    b2 = vetor de vieses de dimensão (n_y, 1)\n",
    "    dJdW1 = matriz de gradientes dos pesos de dimensão para todos exemplos de treinamento (n_h, n_x)\n",
    "    dJdb1 = vetor de gradientes dos vieses de dimensão para todos exemplos de treinamento (n_h, 1)\n",
    "    dJdW2 = matriz de gradientes dos pesos de dimensão para todos exemplos de treinamento (n_y, n_h)\n",
    "    dJdb2 = vetor de gradientes dos vieses de dimensão para todos exemplos de treinamento (n_y, 1)\n",
    "        \n",
    "    Retorna parametros atualizados:\n",
    "    W1 = matriz de pesos de dimensão (n_h, n_x)\n",
    "    b1 = vetor de vieses de dimensão (n_h, 1)\n",
    "    W2 = matriz de pesos de dimensão (n_y, n_h)\n",
    "    b2 = vetor de vieses de dimensão (n_y, 1)\n",
    "    \"\"\"\n",
    "\n",
    "    # Atualização dos parâmetros\n",
    "    ### COMECE AQUI ### (≈ 4 linhas)\n",
    "    #\n",
    "    ### TERMINE AQUI ###\n",
    " \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para testar a sua função `update_parameters()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Nesse momento utilizamos os gradientes dos parâmetros para um único exemplo somente \n",
    "# para podermos testar a função update parâmetros\n",
    "dJdW1 = dW1\n",
    "dJdb1 = db1\n",
    "dJdW2 = dW2\n",
    "dJdb2 = db2\n",
    "\n",
    "W1_n, b1_n, W2_n, b2_n = update_parameters(W1, b1, W2, b2, dJdW1, dJdb1, dJdW2, dJdb2) \n",
    "\n",
    "print(\"W1 = \", W1_n)\n",
    "print(\"b1 = \", b1_n)\n",
    "print(\"W2 = \", W2_n)\n",
    "print(\"b2 = \", b2_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "    W1 =  [[ 0.00606625 -0.00090507]\n",
    "     [ 0.00701601  0.00331644]\n",
    "     [ 0.00774059  0.00088986]\n",
    "     [ 0.00505957  0.00413667]]\n",
    "    b1 =  [[0.00179788]\n",
    "     [0.00160094]\n",
    "     [0.00372676]\n",
    "     [0.00317481]]\n",
    "    W2 =  [[0.00537848 0.00410671 0.00732148 0.0040505 ]]\n",
    "    b2 =  [[0.59999435]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.7 - Integração das tarefas 3.1 a 3.6 na função rna() ####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #9:\n",
    "\n",
    "Programe a sua rede neural na função `rna()`. Inclua tanto a propagação para frente como a retro propagação. A sua rede deve seguir o Algoritmo 1 da Aula 4 - Classificação binária com RNA rasa, que em linhas gerais é o seguinte:\n",
    "\n",
    " `Inicializa parâmetros \n",
    "  for e=1 to n_epocas\n",
    "     zera gradientes\n",
    "     Iniciliza função de custo com zero\n",
    "     for i=1 to m\n",
    "         Calcula a propagação para frente para cada exemplo\n",
    "         Calcula função de erro\n",
    "         Atualiza função de custo\n",
    "         Calcula a propagação para trás para cada exemplo\n",
    "         Acumula os gradientes de cada exemplo nos gradientes de cada parâmetro da rede \n",
    "     Atualiza os parâmetros`\n",
    "\n",
    "**Instruções**: \n",
    "\n",
    "- A sua função `rna()` deve usar as funções programadas anteriormente.\n",
    "- Um comando `for` em python para um contador `i` variando de `0` até `n-1` é implementado por: `for i in range(n):`\n",
    "- Em python para acumular valores em uma variável `dx` pode-se usar `dx  += dx`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: programação da rede neural\n",
    "\n",
    "def rna(X, Y, n_h, num_epocas = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    X = matriz de dados de entrada de dimensão (2, número de exemplos)\n",
    "    Y = vetor com as classes dos exemplos de dimensão (1, número de exemplos)\n",
    "    n_h = número de neurônios da camada escondida\n",
    "    num_epocas = número de épocas\n",
    "    print_cost = se for True, imprime o valor do custo a cada 1000 épocas\n",
    "    \n",
    "    Retorna parâmetros calculados no treinamento:\n",
    "    W1 = matriz de pesos de dimensão (n_h, n_x)\n",
    "    b1 = vetor de vieses de dimensão (n_h, 1)\n",
    "    W2 = matriz de pesos de dimensão (n_y, n_h)\n",
    "    b2 = vetor de vieses de dimensão (n_y, 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    #np.random.seed(3)\n",
    "    n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Inicializa parâmetros. Entradas: \"n_x, n_h, n_y\". Saídas: \"parameters\".\n",
    "    ### COMECE AQUI ### (≈ 1 linha)\n",
    "    #\n",
    "    ### TERMINE AQUI ###\n",
    "    \n",
    "    # Iteração nas épocas \n",
    "    for e in range(num_epocas):\n",
    "\n",
    "        # No início de cada época deve-se inicializar os gradientes dos parâmetros para todos os exemplos com zeros\n",
    "        ### COMECE AQUI ### (≈ 4 linhas)\n",
    "        #dJdW1 \n",
    "        #dJdb1 \n",
    "        #dJdW2 \n",
    "        #dJdb2\n",
    "        ### TERMINE AQUI ###\n",
    "        \n",
    "        # Incializa função de custo\n",
    "        custo = 0\n",
    "        \n",
    "        # Iteração nos exemplos\n",
    "        for i in range(m):\n",
    "            # Propagação para frente. Entradas: X[:,i] e parameters. Saída: za.\n",
    "            ### COMECE AQUI ### (≈ 1 linha)\n",
    "            #z1, a1, z2, a2 = \n",
    "            ## TERMINE AQUI ###\n",
    "            \n",
    "            # Função de erro. Entradas: a2, Y[0][i]. Saída: erro.\n",
    "            ### COMECE AQUI ### (≈ 1 linha)\n",
    "            # Calcula erro usando a função logistica\n",
    "            #erro = \n",
    "            ## TERMINE AQUI ###\n",
    "                     \n",
    "            # Atualiza função de custo somando o erro do exemplo \"i\" e dividindo pelo número total de exemplos \"m\".\n",
    "            ### COMECE AQUI ### (≈ 1 linha)\n",
    "            #custo \n",
    "            ## TERMINE AQUI ###\n",
    " \n",
    "            # Retro-propagação. Entradas: parameters, X[:,i], Y[0][i], za, parameters. Saídas: gradientes.\n",
    "            ### COMECE AQUI ### (≈ 1 linha)\n",
    "            #dW1, db1, dW2, db2 \n",
    "            ## TERMINE AQUI ###\n",
    "            \n",
    "            # Acumula os gradientes de cada exemplo em dJdpar dividindo pelo numero de exemplos. \n",
    "            ### COMECE AQUI ### (≈ 4 linhas)\n",
    "            #dJdW1 \n",
    "            #dJdb1 \n",
    "            #dJdW2 \n",
    "            #dJdb2\n",
    "            ## TERMINE AQUI ###\n",
    " \n",
    "         # Atualização dos parâmetros. Entradas: \"parameters, dJ\". Saídas: \"parameters\".\n",
    "        ### COMECE AQUI ### (≈ 1 linha)\n",
    "        #W1, b1, W2, b2 \n",
    "        ### TERMINE AQUI ###\n",
    "        \n",
    "        # iMPRESSÃO DO CUSTO A CADA 500 épocas\n",
    "        if print_cost and e % 500 == 0:\n",
    "            print (\"Custo após época %i: %f\" %(e, custo))\n",
    " \n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para testar a sua função `rna()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W1, b1, W2, b2 = rna(X, Y, 4, num_epocas=1, print_cost=True)\n",
    "print(\"W1 = \", W1)\n",
    "print(\"b1 = \", b1)\n",
    "print(\"W2 = \", W2)\n",
    "print(\"b2 = \", b2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "    Custo após época 0: 0.693143\n",
    "    W1 =  [[ 0.00526548 -0.00042846]\n",
    "     [ 0.00630294  0.00374085]\n",
    "     [ 0.00608072  0.00187783]\n",
    "     [ 0.00364558  0.00497833]]\n",
    "    b1 =  [[-6.16413329e-08]\n",
    "     [-5.38943520e-08]\n",
    "     [-1.11947354e-07]\n",
    "     [-5.67566995e-08]]\n",
    "    W2 =  [[0.00425463 0.00333027 0.00672357 0.00448863]]\n",
    "    b2 =  [[-1.48910483e-05]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Treinamento e teste da RNA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Previsão das saídas\n",
    "\n",
    "### Exercício #10:\n",
    "\n",
    "Use a sua rede neural para realizar previsões programando na célula abaixo o método `predict()`. \n",
    "\n",
    "Use a propagação para frente para calcular as previsões. Como a saída da rede neural será um valor entre 0 e 1, para definir as classes faz-se:\n",
    "\n",
    "$$previsão = y_{prediction} = \\begin{cases}\n",
    "      1 & \\text{se}\\ saída > 0,5 \\\\\n",
    "      0 & \\text{caso contrário}\n",
    "    \\end{cases}$$\n",
    "    \n",
    "Genericamente, na equação acima pode-se usar no lugar de 0,5 um valor de limiar genérico, assim, tem-se:\n",
    "\n",
    "$$y_{prediction} = \\begin{cases}\n",
    "      1 & \\text{se}\\ saída > limiar \\\\\n",
    "      0 & \\text{caso contrário}\n",
    "    \\end{cases}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: função predict\n",
    "\n",
    "def predict(W1, b1, W2, b2, X):\n",
    "    \"\"\"\n",
    "    Usando os parâmetros calculados no treinamento, prevê a classe para todos os exemplos na matriz X\n",
    "    \n",
    "    Argumentos:\n",
    "    W1 = matriz de pesos de dimensão (n_h, n_x)\n",
    "    b1 = vetor de vieses de dimensão (n_h, 1)\n",
    "    W2 = matriz de pesos de dimensão (n_y, n_h)\n",
    "    b2 = vetor de vieses de dimensão (n_y, 1) \n",
    "    X = matriz de entradas de dimensão (n_x, m)\n",
    "    \n",
    "    Retorna\n",
    "    predictions = vetor de previsões (vermelho: 0 / azul: 1)\n",
    "    \"\"\"\n",
    "    # Incicliza vetor de previsões para os m exemplos \n",
    "    m = X.shape[1]\n",
    "    predictions = np.zeros((m, 1))\n",
    "    \n",
    "    # Calcula as probabilidades usando a propagação para frente e classifica como 0/1 usando um limiar de 0,5.\n",
    "    # utilize um comando de repetição for para percorrer todos os exemplos\n",
    "    ### COMECE AQUI ### (≈ 3 linhas)\n",
    "    # loop for\n",
    "    # for \n",
    "        # calcula a propagação para frente iusando a função foward_propagation\n",
    "        #z1, a1, z2, a2 \n",
    "        \n",
    "        # Calcule a classe prevista pela rede usando o limiar de 0,5\n",
    "        #predictions[i] \n",
    "    ### TERMINE AQUI ###\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute a célula abaixo para testar a sua função `predict()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(W1, b1, W2, b2, X)\n",
    "print(\"Média das previsões = \" + str(np.mean(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**: \n",
    "\n",
    "    Média das previsões = 0.715"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - Treinamento da RNA\n",
    "\n",
    "Agora verifique o desempenho do seu modelo no conjunto de dados, após o treinamento da rede neural com 3.000 épocas. Execute o programa abaixo para testar o seu modelo de uma única camada intermediária com $n_h = 4$ neurônios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Treinamento e excecução da rede neural de uma única camada\n",
    "W1, b1, W2, b2 = rna(X, Y, n_h = 4, num_epocas = 3001, print_cost=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**:\n",
    "\n",
    "    Custo após época 0: 0.693143\n",
    "    Custo após época 500: 0.278122\n",
    "    Custo após época 1000: 0.106389\n",
    "    Custo após época 1500: 0.067668\n",
    "    Custo após época 2000: 0.065335\n",
    "    Custo após época 2500: 0.064553\n",
    "    Custo após época 3000: 0.063936"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - Resultados\n",
    "\n",
    "Execute a célula abaixo para fazer o gráfico dos dados com a fronteira de decisão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define função para fazer gráfico da fronteira de decisão\n",
    "def plot_decision_boundary(model, **args):\n",
    "    # Recupera dados de entrada da função\n",
    "    X = args[\"X\"]\n",
    "    Y = args[\"Y\"]\n",
    "    W1 = args[\"W1\"]\n",
    "    b1 = args[\"b1\"]\n",
    "    W2 = args[\"W2\"]\n",
    "    b2 = args[\"b2\"]\n",
    "    \n",
    "    # Define limites para o gráfico\n",
    "    x_min, x_max = X[0, :].min() - 1, X[0, :].max() + 1\n",
    "    y_min, y_max = X[1, :].min() - 1, X[1, :].max() + 1\n",
    "    \n",
    "    # Gera malha com pontos distanciados de h\n",
    "    h = 0.01\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "    XX = np.c_[xx.ravel(), yy.ravel()].T\n",
    "    \n",
    "    # Calcula função predict para todos os pontos ds malha    \n",
    "    Z = model(W1, b1, W2, b2, XX)\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Faz os gráficos do contorno da fronteira e dos exemplso de treinamento\n",
    "    plt.contourf(xx, yy, Z, cmap=plt.cm.Spectral)\n",
    "    plt.ylabel('x2')\n",
    "    plt.xlabel('x1')\n",
    "    plt.scatter(X[0, :], X[1, :], c=Y, cmap=plt.cm.Spectral)    \n",
    "\n",
    "# Executa função plot_decision_boundary\n",
    "plot_decision_boundary(predict, X=X, Y=Y.ravel(), W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "print(\"Fronteira de decisão da RNA de uma camada escondida com número de neurônios igual a: \" + str(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercício #11:\n",
    "\n",
    "Implemente na célula abaixo o cálculo da exatidão obtida para todos os exemplos de treinamento.  A equação que implementa o cálculo da exatidão éa seguinte:\n",
    "\n",
    "$$ exatidão = 100*(1 - \\frac{1}{m}\\sum_{i=1}^{m} |y_{real}^{(i)} - y_{previsto}^{(i)}|) $$\n",
    "\n",
    "Use as funções `np.abs` e `np.sum` para calcular o módulo de um número e a somatória dos elementos de um vetor.\n",
    "\n",
    "Cuidado com as dimensões das previsões e do vetor de saídas `Y`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: Calculo da exatidão\n",
    "\n",
    "# Calcule as previsões da rede usando a função predict\n",
    "### COMECE AQUI ### (≈ 1 linha)\n",
    "#predictions \n",
    "### TERMINE AQUI ###\n",
    "\n",
    "# Calcule a exatidão obtida pela rede. Para acertar as dimensões use o transposto de predictions\n",
    "### COMECE AQUI ### (≈ 1 linha)\n",
    "#exatidao \n",
    "### TERMINE AQUI ###\n",
    "\n",
    "print('Exatidão: ' + str(exatidao) + ' %')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saída esperada**: \n",
    "\n",
    "    Exatidão: 98.0 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por esse resultado podemos conlcuir que a RNA foi capaz de aprender o padrão de luas! Redes neurais são capazes de aprender fronteiras de decisão muito complexas e não lineares, mesmo com poucos neurônios. \n",
    "\n",
    "Você sabe como calcular o número total de parâmetros dessa RNA?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 - Ajuste do número de neurônios da camada escondida ###\n",
    "\n",
    "Agora, tente outros números de neurônios na camada escondida. Para isso, execute o seguinte programa. Pode levar alguns minutos para executar. Você deve observar comportamentos diferentes para cada número de neurônios na camada escondida.\n",
    "\n",
    "A execução dessa célula vai demorar alguns minutos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 32))\n",
    "hidden_layer_sizes = [2, 3, 4, 5, 20, 40]\n",
    "for i, n_h in enumerate(hidden_layer_sizes):\n",
    "    plt.subplot(5, 2, i+1)\n",
    "    plt.title('Numero de neurônios = %d' % n_h)\n",
    "    W1, b1, W2, b2 = rna(X, Y, n_h, num_epocas = 2000)\n",
    "    plot_decision_boundary(predict, X=X, Y=Y.ravel(), W1=W1, b1=b1, W2=W2, b2=b2)\n",
    "    predictions = predict(W1, b1, W2, b2, X)\n",
    "    exatidao = 100*(1-np.sum(np.abs(Y-predictions.T))/m)\n",
    "    print (\"Exatidão para {} neurônios: {} %\".format(n_h, exatidao))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretação**:\n",
    "- Quanto maior a RNA (maior o número de neurônios) melhor o seu desempenho para aprender os dados de treinamento, até que eventualmente um modelo muito grande apresenta sobre-ajuste dos dados. \n",
    "- O melhor número de camadas escondidas parece ser algo em torno de n_h = 5.\n",
    "- Veremos com mais detalhes como desenvolver RNAs grandes sem problemas de sobre-ajuste dos dados. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 - Desempenho com outros padrões de dados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercício #12:\n",
    "\n",
    "Treine novamente a sua RNA para cada um dos seguintes conjunto de dados. Após o treinamento execute a RNA para calcular as suas previsões e a sua exatidão. \n",
    "\n",
    "Primeiramente execute a célula abaixo para gerar todos os padrões de dados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define função para carregar padrões de pontos no plano\n",
    "def load_extra_datasets():\n",
    "    N = 200\n",
    "    noisy_circles = sklearn.datasets.make_circles(n_samples=N, factor=.5, noise=.3)\n",
    "    noisy_moons = sklearn.datasets.make_moons(n_samples=N, noise=.2)\n",
    "    blobs = sklearn.datasets.make_blobs(n_samples=N, random_state=5, n_features=2, centers=6)\n",
    "    gaussian_quantiles = sklearn.datasets.make_gaussian_quantiles(mean=None, cov=0.5, n_samples=N, n_features=2, n_classes=2, shuffle=True, random_state=None)\n",
    "    no_structure = np.random.rand(N, 2), np.random.rand(N, 2)\n",
    "\n",
    "    return noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure\n",
    "\n",
    "# Conjunto de dados\n",
    "noisy_circles, noisy_moons, blobs, gaussian_quantiles, no_structure = load_extra_datasets()\n",
    "\n",
    "datasets = {\"noisy_circles\": noisy_circles,\n",
    "            \"noisy_moons\": noisy_moons,\n",
    "            \"blobs\": blobs,\n",
    "            \"gaussian_quantiles\": gaussian_quantiles}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para treinar a sua RNA com outros padrões de pontos, modifique o programa abaixo para cada conjunto de dados de cada vez. Use 3001 épocas para cada padrão de dados.\n",
    "\n",
    "Dica: use como base parte do programa do item 4.4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# PARA VOCÊ FAZER: treinar e executar modelo com outros conjuntos de dados\n",
    "\n",
    "### COMECE AQUI\" (≈ 1 linha)\n",
    "#dataset \n",
    "### tERMINE AQUI ###\n",
    "\n",
    "X, Y = datasets[dataset]\n",
    "X, Y = X.T, Y.reshape(1, Y.shape[0])\n",
    "\n",
    "# make blobs binary\n",
    "if dataset == \"blobs\":\n",
    "    Y = Y%2\n",
    "\n",
    "# Visualização dos dados\n",
    "plt.scatter(X[0, :], X[1, :], c=Y.ravel(), s=40, cmap=plt.cm.Spectral);\n",
    "\n",
    "# Número de neurônios da camada escondida\n",
    "n_h = 5\n",
    "\n",
    "### COMECE AQUI ### (≈ 4 linhas)\n",
    "#W1, b1, W2, b2 \n",
    "#plot_decision_boundary\n",
    "#predictions\n",
    "#exatidao \n",
    "### TERMINE AQUI ###\n",
    "\n",
    "print (\"Exatidão para {} neurônios: {} %\".format(n_h, exatidao))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saídas esperadas:** \n",
    "\n",
    "    noyse_circles exatidão 80,5%\n",
    "    bloobs: exatidão 83,0%\n",
    "    gaussian_quantiles: exatidão 99,0%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referência:\n",
    "- http://scs.ryerson.ca/~aharley/neural-networks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color='blue'>\n",
    "    \n",
    "**O que você aprendeu nesse trabalho:**\n",
    "\n",
    "- Construir uma RNA de uma única camada intermediária\n",
    "- Implementar a propagação para frente e a retro propagação\n",
    "- Treinar uma RNA\n",
    "- Observar o impacto de variar o número de neurônios da camada intermediária"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "neural-networks-deep-learning",
   "graded_item_id": "wRuwL",
   "launcher_item_id": "NI888"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
